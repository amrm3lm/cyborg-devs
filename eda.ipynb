{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda plan\n",
    "# show histogram for number of words, hashtags, emojis\n",
    "# word clouds\n",
    "# tf-idf within and between\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data_class1, data_class2, range, value, bins=10, class1_label='Class 1', class2_label='Class 2', save=False, fn='default'):\n",
    "    \"\"\"\n",
    "    Plot a histogram to compare the data from two classes.\n",
    "\n",
    "    Parameters:\n",
    "        data_class1 (list or numpy array): Data for class 1.\n",
    "        data_class2 (list or numpy array): Data for class 2.\n",
    "        bins (int): Number of bins for the histogram. Default is 10.\n",
    "        class1_label (str): Label for class 1 on the legend. Default is 'Class 1'.\n",
    "        class2_label (str): Label for class 2 on the legend. Default is 'Class 2'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "    axs[0].hist(data_class1, bins=bins, range=range,label=class1_label)\n",
    "    axs[1].hist(data_class2, bins=bins, range=range, label=class2_label)\n",
    "    axs[0].title.set_text(class1_label)\n",
    "    axs[1].title.set_text(class2_label)\n",
    "    axs[0].set_xlabel(value)\n",
    "    axs[1].set_xlabel(value)\n",
    "    if save:\n",
    "        plt.savefig(f\"{fn}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_num_words = df[df['spam']==1]['clean_text'].map(lambda x : len(x))\n",
    "ns_num_words = df[df['spam']==0]['clean_text'].map(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(data_class1=s_num_words, range=(0,200),data_class2=ns_num_words, bins=50,value=\"number of words per tweet\", class1_label=\"Spam\", class2_label=\"Not Spam\", save=True, fn=\"hist_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(data_class1=df[df['spam']==1]['num_emojis'], range=(0,20),data_class2=df[df['spam']==0]['num_emojis'], bins=20,value=\"number of emojis\", class1_label=\"Spam\", class2_label=\"Not Spam\", save=True, fn=\"hist_emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(data_class1=df[df['spam']==1]['num_links'], range=(0,5),data_class2=df[df['spam']==0]['num_links'], bins=5,value=\"number of links\", class1_label=\"Spam\", class2_label=\"Not Spam\", save=True, fn=\"hist_links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating word clouds\n",
    "import arabic_reshaper\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df['spam'] == 0]['clean_text'].sample(n=30000).tolist()\n",
    "#sx = ' '.join(sum(x))\n",
    "import ast \n",
    "print(x[0])\n",
    "y = ast.literal_eval(x[0])\n",
    "print(y)\n",
    "words_2d = [ast.literal_eval(j) for j in x]\n",
    "print(words_2d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bidi.algorithm import get_display\n",
    "joined_words = ' '.join(sum(words_2d, []))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def removeWeirdChars(text):\n",
    "    weirdPatterns = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               u\"\\u2069\"\n",
    "                               u\"\\u2066\"\n",
    "                               u\"\\u200c\"\n",
    "                               u\"\\u2068\"\n",
    "                               u\"\\u2067\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return weirdPatterns.sub(r'', text)\n",
    "\n",
    "text = arabic_reshaper.reshape(removeWeirdChars(joined_words))\n",
    "text = get_display(text)\n",
    "wordcloud = WordCloud(\n",
    "    font_path='fonts/NotoNaskhArabic/NotoNaskhArabic-Regular.ttf').generate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"ns_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_word_cloud(pd_series, filename):\n",
    "    \n",
    "    def removeWeirdChars(text):\n",
    "        weirdPatterns = re.compile(\"[\"\n",
    "                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                u\"\\U00002702-\\U000027B0\"\n",
    "                                u\"\\U000024C2-\\U0001F251\"\n",
    "                                u\"\\U0001f926-\\U0001f937\"\n",
    "                                u'\\U00010000-\\U0010ffff'\n",
    "                                u\"\\u200d\"\n",
    "                                u\"\\u2640-\\u2642\"\n",
    "                                u\"\\u2600-\\u2B55\"\n",
    "                                u\"\\u23cf\"\n",
    "                                u\"\\u23e9\"\n",
    "                                u\"\\u231a\"\n",
    "                                u\"\\u3030\"\n",
    "                                u\"\\ufe0f\"\n",
    "                                u\"\\u2069\"\n",
    "                                u\"\\u2066\"\n",
    "                                u\"\\u200c\"\n",
    "                                u\"\\u2068\"\n",
    "                                u\"\\u2067\"\n",
    "                                \"]+\", flags=re.UNICODE)\n",
    "        return weirdPatterns.sub(r'', text)\n",
    "    x = pd_series.tolist()\n",
    "    words_2d = [ast.literal_eval(j) for j in x]\n",
    "    joined_words = ' '.join(sum(words_2d, []))\n",
    "    \n",
    "    text = arabic_reshaper.reshape(removeWeirdChars(joined_words))\n",
    "    text = get_display(text)\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='fonts/NotoNaskhArabic/NotoNaskhArabic-Regular.ttf').generate(text)\n",
    "    wordcloud.to_file(filename)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_word_cloud(df[df['spam'] == 1]['clean_text'], \"spam_wordclout.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "filtered_df = df.loc[(df['spam'] == 1) & df['clean_text'].apply(\n",
    "    lambda words: 'سلمان' in words)]\n",
    "display(filtered_df['raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyborgdevs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
